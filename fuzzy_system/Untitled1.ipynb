{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from deap import creator, base, tools, algorithms\n",
    "from scoop import futures\n",
    "import random\n",
    "import numpy\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read in data from CSV\n",
    "# Data set from https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "dfData = pd.read_csv('heart-statlog.csv')\n",
    "le = LabelEncoder()\n",
    "le.fit(dfData['1'])\n",
    "allClasses = le.transform(dfData['1'])\n",
    "allFeatures = dfData.drop(['1'], axis=1)\n",
    "# Encode the classification labels to numbers\n",
    "# Get classes and one hot encoded featur\n",
    "#X_train = dfData.iloc[0:651,0:11]\n",
    "#X_test= dfData.iloc[651:,0:11]\n",
    "#Y_train = dfData.iloc[0:651,12:]\n",
    "#Y_test= dfData.iloc[651:,12:]\n",
    "X_trainAndTest, X_validation, y_trainAndTest, y_validation = train_test_split(allFeatures, allClasses, test_size=0.20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trainAndTest, y_trainAndTest, test_size=0.20, random_state=42)\n",
    "\n",
    "# Feature subset fitness function\n",
    "#Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFitness(individual, X_train, X_test, y_train, y_test):\n",
    "\n",
    "\t# Parse our feature columns that we don't use\n",
    "\t# Apply one hot encoding to the features\n",
    "\tcols = [index for index in range(len(individual)) if individual[index] == 0]\n",
    "\tX_trainParsed = X_train.drop(X_train.columns[cols], axis=1)\n",
    "\tX_trainOhFeatures = pd.get_dummies(X_trainParsed)\n",
    "\tX_testParsed = X_test.drop(X_test.columns[cols], axis=1)\n",
    "\tX_testOhFeatures = pd.get_dummies(X_testParsed)\n",
    "\n",
    "\t# Remove any columns that aren't in both the training and test sets\n",
    "\tsharedFeatures = set(X_trainOhFeatures.columns) & set(X_testOhFeatures.columns)\n",
    "\tremoveFromTrain = set(X_trainOhFeatures.columns) - sharedFeatures\n",
    "\tremoveFromTest = set(X_testOhFeatures.columns) - sharedFeatures\n",
    "\tX_trainOhFeatures = X_trainOhFeatures.drop(list(removeFromTrain), axis=1)\n",
    "\tX_testOhFeatures = X_testOhFeatures.drop(list(removeFromTest), axis=1)\n",
    "\n",
    "\t# Apply logistic regression on the data, and calculate accuracy\n",
    "\tclf = LogisticRegression()\n",
    "\tclf.fit(X_trainOhFeatures, y_train)\n",
    "\tpredictions = clf.predict(X_testOhFeatures)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "\t# Return calculated accuracy as fitness\n",
    "\treturn (accuracy,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jd\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\deap\\creator.py:141: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n",
      "c:\\users\\jd\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\deap\\creator.py:141: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create Individual\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Create Toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, len(dfData.columns) - 1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Continue filling toolbox...\n",
    "toolbox.register(\"evaluate\", getFitness, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n",
    "toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHof():\n",
    "\n",
    "\t# Initialize variables to use eaSimple\n",
    "\tnumPop = 100\n",
    "\tnumGen = 10\n",
    "\tpop = toolbox.population(n=numPop)\n",
    "\thof = tools.HallOfFame(numPop * numGen)\n",
    "\tstats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "\tstats.register(\"avg\", numpy.mean)\n",
    "\tstats.register(\"std\", numpy.std)\n",
    "\tstats.register(\"min\", numpy.min)\n",
    "\tstats.register(\"max\", numpy.max)\n",
    "\n",
    "\t# Launch genetic algorithm\n",
    "\tpop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=numGen, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "\t# Return the hall of fame\n",
    "\treturn hof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy with all features: \t0.6714285714285714\n",
      "Validation accuracy with all features: \t0.7428571428571429\n",
      "\n",
      "gen\tnevals\tavg     \tstd      \tmin     \tmax     \n",
      "0  \t100   \t0.621429\t0.0283023\t0.528571\t0.671429\n",
      "1  \t66    \t0.640286\t0.0192333\t0.592857\t0.692857\n",
      "2  \t61    \t0.653357\t0.0188565\t0.614286\t0.721429\n",
      "3  \t59    \t0.663429\t0.0200835\t0.592857\t0.721429\n",
      "4  \t66    \t0.671143\t0.0213312\t0.621429\t0.721429\n",
      "5  \t68    \t0.689286\t0.0237654\t0.614286\t0.721429\n",
      "6  \t60    \t0.701214\t0.0237344\t0.614286\t0.721429\n",
      "7  \t54    \t0.711357\t0.0218318\t0.607143\t0.721429\n",
      "8  \t60    \t0.716   \t0.017529 \t0.635714\t0.721429\n",
      "9  \t64    \t0.718857\t0.0116286\t0.635714\t0.721429\n",
      "10 \t46    \t0.716214\t0.0181506\t0.635714\t0.721429\n",
      "\n",
      "---Optimal Feature Subset(s)---\n",
      "\n",
      "Percentile: \t\t\t0.9523809523809523\n",
      "Validation Accuracy: \t\t0.7885714285714286\n",
      "Individual: \t[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "Number Features In Subset: \t5\n",
      "Feature Subset: ['63', '145', '233', '3', '6']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jd\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\matplotlib\\figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feature subset fitness function\n",
    "\n",
    "\n",
    "#========DEAP GLOBAL VARIABLES (viewable by SCOOP)========\n",
    "\n",
    "\n",
    "#========\n",
    "\n",
    "\n",
    "\n",
    "def getMetrics(hof):\n",
    "\n",
    "\t# Get list of percentiles in the hall of fame\n",
    "\tpercentileList = [i / (len(hof) - 1) for i in range(len(hof))]\n",
    "\t\n",
    "\t# Gather fitness data from each percentile\n",
    "\ttestAccuracyList = []\n",
    "\tvalidationAccuracyList = []\n",
    "\tindividualList = []\n",
    "\tfor individual in hof:\n",
    "\t\ttestAccuracy = individual.fitness.values\n",
    "\t\tvalidationAccuracy = getFitness(individual, X_trainAndTest, X_validation, y_trainAndTest, y_validation)\n",
    "\t\ttestAccuracyList.append(testAccuracy[0])\n",
    "\t\tvalidationAccuracyList.append(validationAccuracy[0])\n",
    "\t\tindividualList.append(individual)\n",
    "\ttestAccuracyList.reverse()\n",
    "\tvalidationAccuracyList.reverse()\n",
    "\treturn testAccuracyList, validationAccuracyList, individualList, percentileList\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\t'''\n",
    "\tFirst, we will apply logistic regression using all the features to acquire a baseline accuracy.\n",
    "\t'''\n",
    "\tindividual = [1 for i in range(len(allFeatures.columns))]\n",
    "\ttestAccuracy = getFitness(individual, X_train, X_test, y_train, y_test)\n",
    "\tvalidationAccuracy = getFitness(individual, X_trainAndTest, X_validation, y_trainAndTest, y_validation)\n",
    "\tprint('\\nTest accuracy with all features: \\t' + str(testAccuracy[0]))\n",
    "\tprint('Validation accuracy with all features: \\t' + str(validationAccuracy[0]) + '\\n')\n",
    "\n",
    "\t'''\n",
    "\tNow, we will apply a genetic algorithm to choose a subset of features that gives a better accuracy than the baseline.\n",
    "\t'''\n",
    "\thof = getHof()\n",
    "\ttestAccuracyList, validationAccuracyList, individualList, percentileList = getMetrics(hof)\n",
    "\n",
    "\t# Get a list of subsets that performed best on validation data\n",
    "\tmaxValAccSubsetIndicies = [index for index in range(len(validationAccuracyList)) if validationAccuracyList[index] == max(validationAccuracyList)]\n",
    "\tmaxValIndividuals = [individualList[index] for index in maxValAccSubsetIndicies]\n",
    "\tmaxValSubsets = [[list(allFeatures)[index] for index in range(len(individual)) if individual[index] == 1] for individual in maxValIndividuals]\n",
    "\n",
    "\tprint('\\n---Optimal Feature Subset(s)---\\n')\n",
    "\tfor index in range(len(maxValAccSubsetIndicies)):\n",
    "\t\tprint('Percentile: \\t\\t\\t' + str(percentileList[maxValAccSubsetIndicies[index]]))\n",
    "\t\tprint('Validation Accuracy: \\t\\t' + str(validationAccuracyList[maxValAccSubsetIndicies[index]]))\n",
    "\t\tprint('Individual: \\t' + str(maxValIndividuals[index]))\n",
    "\t\tprint('Number Features In Subset: \\t' + str(len(maxValSubsets[index])))\n",
    "\t\tprint('Feature Subset: ' + str(maxValSubsets[index]))\n",
    "\n",
    "\t'''\n",
    "\tNow, we plot the test and validation classification accuracy to see how these numbers change as we move from our worst feature subsets to the \n",
    "\tbest feature subsets found by the genetic algorithm.\n",
    "\t'''\n",
    "\t# Calculate best fit line for validation classification accuracy (non-linear)\n",
    "\ttck = interpolate.splrep(percentileList, validationAccuracyList, s=5.0)\n",
    "\tynew = interpolate.splev(percentileList, tck)\n",
    "\n",
    "\te = plt.figure(1)\n",
    "\tplt.plot(percentileList, validationAccuracyList, marker='o', color='r')\n",
    "\tplt.plot(percentileList, ynew, color='b')\n",
    "\tplt.title('Validation Set Classification Accuracy vs. \\n Continuum with Cubic-Spline Interpolation')\n",
    "\tplt.xlabel('Population Ordered By Increasing Test Set Accuracy')\n",
    "\tplt.ylabel('Validation Set Accuracy')\n",
    "\te.show()\n",
    "\n",
    "\tf = plt.figure(2)\n",
    "\tplt.scatter(percentileList, validationAccuracyList)\n",
    "\tplt.title('Validation Set Classification Accuracy vs. Continuum')\n",
    "\tplt.xlabel('Population Ordered By Increasing Test Set Accuracy')\n",
    "\tplt.ylabel('Validation Set Accuracy')\n",
    "\tf.show()\n",
    "\n",
    "\tg = plt.figure(3)\n",
    "\tplt.scatter(percentileList, testAccuracyList)\n",
    "\tplt.title('Test Set Classification Accuracy vs. Continuum')\n",
    "\tplt.xlabel('Population Ordered By Increasing Test Set Accuracy')\n",
    "\tplt.ylabel('Test Set Accuracy')\n",
    "\tg.show()\n",
    "\n",
    "\tinput()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
